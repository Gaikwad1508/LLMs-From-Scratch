{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3cd01b",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e887bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt', 'r', encoding = 'utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b05efba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"\\'()/]|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2e9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c29a7",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2bb69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47ebafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token : integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd9ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1838d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"(\\')/]|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        #replace spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([./,:;\\'()?\"])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a8d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fe991c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c32554",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#let's try something which is not present in the vocabulary\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like Biryani\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)/]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m---> 10\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "#let's try something which is not present in the vocabulary\n",
    "\n",
    "text2 = \"Hello, do you like Biryani\"\n",
    "tokenizer.encode(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2948cc",
   "metadata": {},
   "source": [
    "### Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7faf8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext>|', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c583e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bbe90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext>|', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for item in list(vocab.items())[-5:]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ebead3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;()?/\"!_]|--|\\s)', text)         # re.sub() = regex replace; \\s+ = one or more spaces; ([,.:;?!\"()']) = capture group for punctuation; r'\\1' = keeps punctuation only, removes space before it; text = input string to clean\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else '<|unk|>' for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids)\n",
    "\n",
    "        #replace spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([,.;:/?!\"\\'()])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa08660",
   "metadata": {},
   "source": [
    "# üìò Regex in Python: `re.sub()`, `re.split()`, and Escape Sequences\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. `re.sub()` ‚Äì Substitution with Regex\n",
    "\n",
    "### üîß Syntax:\n",
    "```python\n",
    "re.sub(pattern, replacement, string)\n",
    "\n",
    "text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "\n",
    "üîç Explanation (inline breakdown):\n",
    "re.sub() ‚Üí Performs regex-based substitution.\n",
    "\n",
    "r'...' ‚Üí Raw string to handle backslashes properly.\n",
    "\n",
    "\\s+ ‚Üí Matches one or more whitespace characters.\n",
    "\n",
    "([,.:;?!\"()']) ‚Üí Capture group matching any one punctuation character.\n",
    "\n",
    "\\1 ‚Üí Refers to the first capture group (i.e., the punctuation).\n",
    "\n",
    "‚úÖ Removes spaces before punctuation while keeping the punctuation.\n",
    "\n",
    "text ‚Üí The input string being modified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344e8e0",
   "metadata": {},
   "source": [
    "üîπ Escape Sequences in Regex\n",
    "\n",
    "| Pattern | Matches                       |\n",
    "| ------- | ----------------------------- |\n",
    "| `\\d`    | Digit (0‚Äì9)                   |\n",
    "| `\\s`    | Whitespace                    |\n",
    "| `\\w`    | Word char (a‚Äìz, A‚ÄìZ, 0‚Äì9, \\_) |\n",
    "| `\\\\`    | Literal backslash `\\`         |\n",
    "| `\\.`    | Literal dot `.`               |\n",
    "\n",
    "üîπ Literal Backslash Match\n",
    "python\n",
    "Copy code\n",
    "pattern = r'\\\\'\n",
    "Use raw strings (r'') to avoid Python escaping.\n",
    "\n",
    "Double \\\\ matches one literal backslash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30bb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like Biryani? <|endoftext|> In the sunlit terracesf of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = 'Hello, do you like Biryani?'\n",
    "text2 = 'In the sunlit terracesf of the palace.'\n",
    "\n",
    "text = ' <|endoftext|> '.join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59555a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131,\n",
       " 5,\n",
       " 355,\n",
       " 1126,\n",
       " 628,\n",
       " 1131,\n",
       " 10,\n",
       " 1131,\n",
       " 55,\n",
       " 988,\n",
       " 956,\n",
       " 1131,\n",
       " 722,\n",
       " 988,\n",
       " 1131,\n",
       " 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2910070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like <|unk|>? <|unk|> In the sunlit <|unk|> of the <|unk|>.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213b1d5",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ea3f4",
   "metadata": {},
   "source": [
    "BPE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e349fe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc50f334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print('tiktoken version:', importlib.metadata.version('tiktoken'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9ac41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 347, 9045, 3216, 30, 220, 50256, 262, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like Biryani? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd6b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like Biryani? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "173b2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 38130, 220, 959]\n",
      "Akwirwzeb ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirwzeb ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec219e09",
   "metadata": {},
   "source": [
    "### Data Sampling with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6909ff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt', 'r', encoding = 'utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15e8bbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[290,\n",
       " 4920,\n",
       " 2241,\n",
       " 287,\n",
       " 257,\n",
       " 4489,\n",
       " 64,\n",
       " 319,\n",
       " 262,\n",
       " 34686,\n",
       " 41976,\n",
       " 13,\n",
       " 357,\n",
       " 10915,\n",
       " 314,\n",
       " 2138,\n",
       " 1807,\n",
       " 340,\n",
       " 561,\n",
       " 423,\n",
       " 587,\n",
       " 10598,\n",
       " 393,\n",
       " 28537,\n",
       " 2014,\n",
       " 198,\n",
       " 198,\n",
       " 1,\n",
       " 464,\n",
       " 6001,\n",
       " 286,\n",
       " 465,\n",
       " 13476,\n",
       " 1,\n",
       " 438,\n",
       " 5562,\n",
       " 373,\n",
       " 644,\n",
       " 262,\n",
       " 1466,\n",
       " 1444,\n",
       " 340,\n",
       " 13,\n",
       " 314,\n",
       " 460,\n",
       " 3285,\n",
       " 9074,\n",
       " 13,\n",
       " 46606,\n",
       " 536]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "enc_sample[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ead02a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "814093e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5b3ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382ca80",
   "metadata": {},
   "source": [
    "### Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbd01b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_length = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):        #here batch size is number of batches the model process before changing the models parameters, and num_workers are about number or threads working, max_length is context size.\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d962b1a",
   "metadata": {},
   "source": [
    "## Implementing a simplified self attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b705018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],\n",
    "    [0.55, 0.87, 0.66],\n",
    "    [0.57, 0.85, 0.64],\n",
    "    [0.22, 0.58, 0.33],\n",
    "    [0.77, 0.25, 0.10],\n",
    "    [0.05, 0.80, 0.55]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db0927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "#calculating attention scores for all tokens wrt second token\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, xi in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, xi)\n",
    "\n",
    "print(attn_scores_2)\n",
    "\n",
    "# attn_scores_2 = torch.matmul(query, inputs.T)\n",
    "# print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61601b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attention_weight_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attention_weight_2_tmp)\n",
    "print(\"sum: \", attention_weight_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c4c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
